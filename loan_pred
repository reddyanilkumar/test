



In [42]:



import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn import preprocessing
import pandas as pd




In [43]:



train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')




In [44]:



category_variables = {'Gender','Married','Dependents','Self_Employed','Property_Area'}
numeric_variables = {'LoanAmount','Loan_Amount_Term','Credit_History'}
for var in numeric_variables:
    train[var] = train[var].fillna(value = train[var].mean())
    test[var] = test[var].fillna(value = test[var].mean())
train['Credibility'] = train['ApplicantIncome'] / train['LoanAmount']
test['Credibility'] = test['ApplicantIncome'] / test['LoanAmount']




In [45]:



train = train.fillna(value = -999)
test = test.fillna(value = -999)
for var in category_variables:
    lb = preprocessing.LabelEncoder()
    full_data = pd.concat((train[var],test[var]),axis=0).astype('str')
    lb.fit( full_data )
    train[var] = lb.transform(train[var].astype('str'))
    test[var] = lb.transform(test[var].astype('str'))




In [46]:



features = {'Credibility','Gender','Married','Dependents','Self_Employed','Property_Area','ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History'}

#features = ['Credit_History', 'ApplicantIncome', 'LoanAmount', 'CoapplicantIncome']




In [47]:



Loan_Status_map = {'Y': 1, 'N': 0}

train['Loan_Status'] = train.Loan_Status.map(Loan_Status_map)




In [48]:



x = train[list(features)]
y = train['Loan_Status']
x_test = test[list(features)]




In [49]:



y.unique()





Out[49]:

array([1, 0], dtype=int64)



In [50]:



from sklearn.cross_validation import train_test_split
X_train,X_test,Y_train,y_test = train_test_split(x,y)




In [51]:



rf1 = RandomForestClassifier(n_estimators=500,oob_score = True, max_features = "auto",random_state=30,min_samples_split=2, min_samples_leaf=2)




In [52]:



rf1.fit(X_train, Y_train)





Out[52]:

RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_samples_leaf=2, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,
            oob_score=True, random_state=30, verbose=0, warm_start=False)



In [55]:



pred_forest = rf1.predict(X_test)




In [110]:



from sklearn.ensemble import GradientBoostingClassifier
#rf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=42)




In [13]:



from sklearn.ensemble import AdaBoostClassifier
rf = AdaBoostClassifier(n_estimators=500, learning_rate=0.01, random_state=42)




In [14]:



rf.fit(x_train, y_train)





Out[14]:

AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,
          learning_rate=0.01, n_estimators=500, random_state=42)



In [15]:



pred_ada= rf.predict(x_test)




In [62]:



rf1.score(pred_forest,y_test)






---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-62-a693f108f4b5> in <module>()
----> 1 rf1.score(pred_forest,y_test)

C:\Users\anilkumar_reddy01\AppData\Local\Continuum\Anaconda\lib\site-packages\sklearn\base.pyc in score(self, X, y, sample_weight)
    293         """
    294         from .metrics import accuracy_score
--> 295         return accuracy_score(y, self.predict(X), sample_weight=sample_weight)
    296 
    297 

C:\Users\anilkumar_reddy01\AppData\Local\Continuum\Anaconda\lib\site-packages\sklearn\ensemble\forest.pyc in predict(self, X)
    460         # for 1d.
    461         X = check_array(X, ensure_2d=False, accept_sparse="csr")
--> 462         proba = self.predict_proba(X)
    463 
    464         if self.n_outputs_ == 1:

C:\Users\anilkumar_reddy01\AppData\Local\Continuum\Anaconda\lib\site-packages\sklearn\ensemble\forest.pyc in predict_proba(self, X)
    511                              backend="threading")(
    512             delayed(_parallel_helper)(e, 'predict_proba', X, check_input=False)
--> 513             for e in self.estimators_)
    514 
    515         # Reduce

C:\Users\anilkumar_reddy01\AppData\Local\Continuum\Anaconda\lib\site-packages\sklearn\externals\joblib\parallel.pyc in __call__(self, iterable)
    657             self._iterating = True
    658             for function, args, kwargs in iterable:
--> 659                 self.dispatch(function, args, kwargs)
    660 
    661             if pre_dispatch == "all" or n_jobs == 1:

C:\Users\anilkumar_reddy01\AppData\Local\Continuum\Anaconda\lib\site-packages\sklearn\externals\joblib\parallel.pyc in dispatch(self, func, args, kwargs)
    404         """
    405         if self._pool is None:
--> 406             job = ImmediateApply(func, args, kwargs)
    407             index = len(self._jobs)
    408             if not _verbosity_filter(index, self.verbose):

C:\Users\anilkumar_reddy01\AppData\Local\Continuum\Anaconda\lib\site-packages\sklearn\externals\joblib\parallel.pyc in __init__(self, func, args, kwargs)
    138         # Don't delay the application, to avoid keeping the input
    139         # arguments in memory
--> 140         self.results = func(*args, **kwargs)
    141 
    142     def get(self):

C:\Users\anilkumar_reddy01\AppData\Local\Continuum\Anaconda\lib\site-packages\sklearn\ensemble\forest.pyc in _parallel_helper(obj, methodname, *args, **kwargs)
    104 def _parallel_helper(obj, methodname, *args, **kwargs):
    105     """Private helper to workaround Python 2 pickle limitations"""
--> 106     return getattr(obj, methodname)(*args, **kwargs)
    107 
    108 

C:\Users\anilkumar_reddy01\AppData\Local\Continuum\Anaconda\lib\site-packages\sklearn\tree\tree.pyc in predict_proba(self, X, check_input)
    588                              " match the input. Model n_features is %s and "
    589                              " input n_features is %s "
--> 590                              % (self.n_features_, n_features))
    591 
    592         proba = self.tree_.predict(X)

ValueError: Number of features of the model must  match the input. Model n_features is 11 and  input n_features is 154 



In [61]:



pred_forest.shape





Out[61]:

(154,)



In [28]:



from sklearn.cross_validation import cross_val_score
scores = cross_val_score(rf1,x,y,cv=25)
print (scores)
print (np.mean(scores))






[ 0.84        0.76        0.84        0.84        0.76        0.76        0.76
  0.8         0.72        0.8         0.64        0.84        0.8         0.76
  0.8         0.88        0.96        0.875       0.79166667  0.83333333
  0.75        0.83333333  0.95652174  0.7826087   0.86956522]
0.81008115942




In [23]:



from collections import Counter
import numpy as np

def majority_voting(preds):
    """
    Given an array of predictions from various classifiers
    return single array with ensemble of predictions based on
    simple majority voting
    
    Input: list of list [[y1, y2, y3, ..], [y1, y2, y3, ...], ..] 
    Output: final prediction [y1, y2, y3, ..]
    """
    length = [len(pred) for pred in preds]
    
    if len(set(length)) != 1:
        raise ValueError('Predictions must be of the same length')
    
    pred_matrix = np.matrix(preds)
    ensemble_preds = []
    
    for i in range(len(preds[0])):
        pred_column = np.array(pred_matrix[:, i]).ravel()
        common_pred = Counter(pred_column)
        most_common = common_pred.most_common()[0][0]
        
        ensemble_preds.append(most_common)
    
    return ensemble_preds




In [24]:



ensemble_preds = majority_voting([pred_forest, pred_ada])




In [25]:



test['Loan_Status'] = ensemble_preds
test.to_csv('sub1.csv',columns=['Loan_ID','Loan_Status'],index=False)




In [ ]:



 

https://github.com/muthu1086
http://machinelearningmastery.com/improve-model-accuracy-with-data-pre-processing/
http://mlwave.com/kaggle-ensembling-guide/
http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/
http://mlwave.com/how-we-won-3rd-prize-in-crowdanalytix-copd-competition/
http://machinelearningmastery.com/how-to-choose-the-right-test-options-when-evaluating-machine-learning-algorithms/
http://scott.fortmann-roe.com/docs/BiasVariance.html
